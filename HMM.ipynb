{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' TO-DO for project 4:\n",
    "        Change the emission probabilities,\n",
    "        cluster words, then when one word in a cluster receives uncertain tag,\n",
    "        add +1 to count of all words in that cluster,\n",
    "        also remember to add count to corresponding tag so probabilities still sum to 1\n",
    "'''\n",
    "\n",
    "import baseline\n",
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import baseline as bl\n",
    "from nltk import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cluster_dicts(inpt):\n",
    "    \n",
    "    ''' Hi Liane, just a few notes\n",
    "    \n",
    "    word_dict is the dictionary of {word: bitstring} so that when we come across a word in training,\n",
    "        we can find out what it's cluster bitstring is.\n",
    "    Then, with the bitstring, we can use it as a key in cluster_dict to get a list of all other words in the cluster\n",
    "    \n",
    "    Then, with the list of other words, we also add a bigram of (tag, word) to our list in the next function\n",
    "        that eventually gets turned in emission probabilities.\n",
    "    This way, we only make code to add more bigrams to our list. We don't actually need to change anything else.\n",
    "    I have more notes about this in the function below\n",
    "    '''\n",
    "    \n",
    "    with open(inpt,'rb') as tsvin:\n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        \n",
    "        word_dict = {} # {word: bit string for cluster, ...}\n",
    "        cluster_dict = {} # {bit string: [word in cluster, word, word,...], ...}\n",
    "        \n",
    "        for row in tsvin:\n",
    "            bitstring = row[0]\n",
    "            word = row[1] \n",
    "            word_dict[word] = bitstring\n",
    "            if cluster_dict.get(bitstring):\n",
    "                cluster_dict[bitstring].append(word)\n",
    "            else:\n",
    "                cluster_dict[bitstring] = [word]\n",
    "        \n",
    "    return word_dict, cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_files(input_path, word_bits, clusters):\n",
    "    \n",
    "    ''' Iterate through all files in a given training path and construct two lists,\n",
    "    one of the tags in order and another of (tag, word) bigrams\n",
    "    \n",
    "    Hi again, TO-DO:\n",
    "        When we come across a word with a B or I tag, we find out what it's bitstring is.\n",
    "        Then with the bitstring we find the list of other words in the same cluster.\n",
    "        Then, with the list of cluster words, we add a bigram of (tag, word) for each word to the tag_words list\n",
    "            --This is how we add a count for each word in the cluster without having to modify other code\n",
    "        \n",
    "        Let me know if you have any questions.  Hope you're having a good break!\n",
    "    '''\n",
    "    \n",
    "    directory = os.path.join(input_path)\n",
    "    # print directory\n",
    "    \n",
    "    corpus = []\n",
    "\n",
    "    thirds = [] # the third element in each row of each doc\n",
    "    thirds_count = []\n",
    "    tag_words = []\n",
    "    tag_threshold = 1 # use this and a np.random.uniform() comparison to omit certain fractions of documents from training\n",
    "    validation_files = {}\n",
    "    validation_threshold = 0.9 # use to extract 1-threshold documents for validation\n",
    "    added_tags = [] # same as thirds but also includes added cluster words\n",
    "    add_tag_threshold = 0.15\n",
    "    \n",
    "    for root,dirs,files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\") and np.random.uniform() < validation_threshold:\n",
    "                temp_thirds = []\n",
    "                temp_tag_words = []\n",
    "                \n",
    "                with open(directory+file,'rb') as tsvin:\n",
    "                    tsvin = csv.reader(tsvin, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "                    \n",
    "                    sentence_thirds = []\n",
    "                    sentence_tag_words = []\n",
    "                    \n",
    "                    for row in tsvin:\n",
    "                        \n",
    "                        if row != []:\n",
    "                            corpus.append(row[0])\n",
    "                            temp_thirds.append(row[2])\n",
    "                            sentence_thirds.append(row[2])\n",
    "                            temp_tag_words.append((row[2],row[0]))\n",
    "                            sentence_tag_words.append((row[2],row[0]))\n",
    "\n",
    "                            #When we come across a word with a B or I tag, we find out what its bitstring is.\n",
    "                            #Then with the bitstring we find the list of other words in the same cluster.\n",
    "                            #Then, with the list of cluster words, we add a bigram of (tag, word) for each word to the tag_words list\n",
    "                            if row[2] != 'O' and row[0] in word_bits and np.random.uniform < add_tag_threshold:\n",
    "                                bit_string = word_bits[row[0]]\n",
    "                                other_words = clusters[bit_string]\n",
    "\n",
    "                                for word in other_words:\n",
    "                                    temp_tag_words.append((row[2],word))\n",
    "                                    added_tags.append(word)\n",
    "                                    thirds_count.append(row[2])\n",
    "                            thirds_count.append(row[2])\n",
    "\n",
    "                        else:\n",
    "                            if 'B' in set(sentence_thirds):\n",
    "                                temp_thirds += (2 * sentence_thirds)\n",
    "                                temp_tag_words += (2 * sentence_tag_words)\n",
    "                            sentence_thirds = []\n",
    "                            sentence_tag_words = []\n",
    "                \n",
    "                # always add if 'B' is in the temp_thirds list\n",
    "                if 'B' in set(temp_thirds):\n",
    "                    thirds += temp_thirds\n",
    "                    tag_words += temp_tag_words\n",
    "                elif np.random.uniform() < tag_threshold: # if no uncertain cues, include if below threshold\n",
    "                    thirds += temp_thirds\n",
    "                    tag_words += temp_tag_words\n",
    "                    \n",
    "            else:\n",
    "                with open(directory+file,'rb') as tsvin:\n",
    "                    tsvin = csv.reader(tsvin, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "                    \n",
    "                    temp_sentence = []\n",
    "                    sentences = []\n",
    "                    sent_tags = []\n",
    "                    tags = []\n",
    "                    \n",
    "                    for row in tsvin: \n",
    "                        if row != []:\n",
    "                            temp_sentence.append(row[0])\n",
    "                            sent_tags.append(row[2])\n",
    "                        else:\n",
    "                            sentences.append(temp_sentence)\n",
    "                            tags.append(sent_tags)\n",
    "                            sent_tags = []\n",
    "                            temp_sentence = []\n",
    "                \n",
    "                validation_files[file] = (sentences, tags)\n",
    "                    \n",
    "    return thirds, thirds_count, tag_words, added_tags, validation_files, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bigrams(input_list):\n",
    "    \n",
    "    ''' Create a list of bigrams from an input list '''\n",
    "    \n",
    "    return zip(input_list, input_list[1:])\n",
    "\n",
    "def bigram_probs(bigrams, prefixes):\n",
    "    \n",
    "    ''' Calculate the bigram probabilities for an input list of tuples and list of the corresponding prefixes '''\n",
    "    \n",
    "    prefix_counts = Counter(prefixes)\n",
    "\n",
    "    bigram_probs = {}\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    for bigram in bigram_counts:\n",
    "#         bigram_probs[bigram] = (bigram_counts[bigram] + 1) / (float(prefix_counts[bigram[0]]) + len(bigram_counts))\n",
    "        bigram_probs[bigram] = (bigram_counts[bigram]) / (float(prefix_counts[bigram[0]]))\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Viterbi, will take in list of strings (a sentence in a document), it will take in transition and emission probs\n",
    "# output a list of tags that is as long as the input sequence\n",
    "\n",
    "def viterbi(t_probs,e_probs,i_probs,states,sentence, train_length):\n",
    "    state_indices = range(len(states))\n",
    "\n",
    "    #initialize probability for first observation:\n",
    "    item_probs = []\n",
    "    for state in states:\n",
    "        try:\n",
    "            item_probs.append(i_probs[state]*e_probs[state,sentence[0]])\n",
    "        except KeyError:\n",
    "            item_probs.append(i_probs[state] * 1 / float(train_length))\n",
    "\n",
    "    path = states   # initialize paths to state names\n",
    "\n",
    "    for word in sentence[1:]:\n",
    "        next_item_probs = []\n",
    "        for i in state_indices:\n",
    "            temp = [item_probs[i]*t_probs[states[i],states[j]] for j in state_indices]\n",
    "            next_item_probs.append(temp)\n",
    "            \n",
    "        new_path=[]\n",
    "        for i in state_indices:\n",
    "            probs_into = [next_item_probs[j][i] for j in state_indices]\n",
    "            try:\n",
    "                item_probs[i] = max(probs_into)*e_probs[(states[i],word)]\n",
    "            except KeyError:\n",
    "                item_probs[i] = max(probs_into)*(1 / float(train_length))\n",
    "                \n",
    "            new_path.append(path[np.argmax(probs_into)] + states[i])\n",
    "        path = new_path\n",
    "    \n",
    "    return path[np.argmax(item_probs)], max(item_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To edit test documents:\n",
    "# Need list of words (first item in each row), including where spaces are so that can write corresponding output\n",
    "\n",
    "# function to make list of sequences from document (read from test-public, test-private folders)\n",
    "# function to write these lists to the edited version of an input document (write to test-public-edited, etc.)\n",
    "\n",
    "def get_sequences(input_path, transition, emission, initial_p, states, length):\n",
    "    with open(input_path, 'rb') as test:\n",
    "        test = csv.reader(test, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        all_words = []\n",
    "    \n",
    "        for row in test:\n",
    "            if row == []:\n",
    "                all_words.append(\"\")\n",
    "            else:\n",
    "                all_words.append(row[0])\n",
    "        \n",
    "        sentences = []\n",
    "        temp = []\n",
    "        i = 0\n",
    "        for i in range(len(all_words)):\n",
    "            if all_words[i] != '':\n",
    "                temp.append(all_words[i])\n",
    "            else:\n",
    "                sentences.append(temp)\n",
    "                temp = []\n",
    "        \n",
    "        sequences = []\n",
    "        for sent in sentences:\n",
    "            sequences.append(list(viterbi(transition, emission, initial_p, states, sent, length)[0]))\n",
    "            \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_sequences(sequences, input_path, output_path):\n",
    "    with open(input_path,'rb') as tsvin, open(output_path, 'wb') as tsvout:\n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        tsvout = csv.writer(tsvout, delimiter='\\t', quoting=csv.QUOTE_NONE, quotechar='')\n",
    "        \n",
    "        sentence_counter = 0\n",
    "        word_counter = 0\n",
    "        \n",
    "        for row in tsvin:\n",
    "            if row == []:\n",
    "                tsvout.writerow(\"\")\n",
    "                sentence_counter += 1\n",
    "                word_counter = 0\n",
    "            else:\n",
    "                tsvout.writerow([row[0], row[1], sequences[sentence_counter][word_counter]])\n",
    "                word_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_indices(file_path):\n",
    "    directory = os.path.join(file_path)\n",
    "    \n",
    "    sentence_counter = 0\n",
    "    index_counter = 0\n",
    "    sentence_indices = []\n",
    "    word_indices = []\n",
    "    uncertain_count = 0\n",
    "    \n",
    "    for root,dirs,files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(file_path+file, 'rb') as doc:\n",
    "                    doc = csv.reader(doc, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "                    \n",
    "                    for row in doc:\n",
    "                        if row != []:\n",
    "                            if row[2] != 'O':\n",
    "                                word_indices.append(index_counter)\n",
    "                                uncertain_count += 1\n",
    "                            index_counter += 1\n",
    "                        else:\n",
    "                            if uncertain_count > 0:\n",
    "                                sentence_indices.append(sentence_counter)\n",
    "                            uncertain_count = 0\n",
    "                            sentence_counter += 1\n",
    "    return word_indices, sentence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Create cluster dicts '''\n",
    "\n",
    "# TO-DO: we should compare cluster dicts from different files\n",
    "\n",
    "cluster_file = 'paths_1000_min50.txt'\n",
    "word_bit, clusters = create_cluster_dicts(cluster_file)\n",
    "# clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On trial 1...\n",
      "On trial 2...\n",
      "On trial 3...\n",
      "On trial 4...\n",
      "On trial 5...\n",
      "On trial 6...\n",
      "On trial 7...\n",
      "On trial 8...\n",
      "On trial 9...\n",
      "On trial 10...\n",
      "\n",
      "Calculated over 10 trials ---------\n",
      "Average true positives: 248.9\n",
      "Average false positives: 677.7\n",
      "Average false negatives: 176.0\n",
      "Percentage of correct tags: 0.93446347431\n",
      "Precision: 0.26164874552\n",
      "Recall: 0.623931623932\n",
      "F-score: 0.368686868687\n"
     ]
    }
   ],
   "source": [
    "''' Train and validate model over multiple trials '''\n",
    "\n",
    "trials = 10\n",
    "total_tp = 0.\n",
    "total_fp = 0.\n",
    "total_fn = 0.\n",
    "\n",
    "for i in range(trials):\n",
    "    print \"On trial %s...\" % (i + 1)\n",
    "    \n",
    "    ''' Train the model '''\n",
    "    \n",
    "    # Arguments to pass into viterbi\n",
    "\n",
    "    [tags, tag_counts, tag_words, add_tags, val_set, corpus] = iterate_files('train-edited/', word_bit, clusters)\n",
    "    tag_bigrams = find_bigrams(tags)\n",
    "    trained_len = len(tag_words)\n",
    "\n",
    "    # transition probabilities\n",
    "    T = bigram_probs(tag_bigrams, tags)\n",
    "    T[('O', 'I')] = 1 - T[('O', 'O')] - T[('O', 'B')]\n",
    "\n",
    "    # dictionary of P(word|tag)\n",
    "    E = bigram_probs(tag_words, tag_counts)\n",
    "\n",
    "    # Initial probabilities\n",
    "    bio_counts = Counter(tags)\n",
    "    bio_total = sum(bio_counts.values())\n",
    "    initial = {}\n",
    "    for item in bio_counts:\n",
    "        initial[item] = bio_counts[item] / float(bio_total)\n",
    "\n",
    "\n",
    "    state_names=['B','I','O']\n",
    "    viterbi(T,E,initial,state_names,['Parviz','Yahaghi',\"'s\",'most','widely','distributed','recordings', 'outside','Iran','is'], trained_len)\n",
    "\n",
    "\n",
    "\n",
    "    ''' \n",
    "        Validate model\n",
    "\n",
    "            Segment the training documents, only train on N portion of them\n",
    "                -this is just our existing procedure, but on a smaller number of documents\n",
    "            Then, run our algorithm on the remaining 1-N portion of documents\n",
    "                Preserve the original training documents for comparison,\n",
    "                but also generate our own model-edited version of the document\n",
    "            Then, compare our model-generated file to the original training document to see score:\n",
    "                (P,R,F=(2PR)/(P+R)) or just %correct?\n",
    "\n",
    "\n",
    "            When we go to generate our training lists, randomly pull out a number of documents\n",
    "                to serve as the validation set.\n",
    "            Record which documents we reserve for validation, record the sequence of tags from that document\n",
    "            Run Viterbi on the documents, compare the output to the output of the step above\n",
    "            Calculate performance metrics\n",
    "\n",
    "\n",
    "            For each document in validation, record the list of tags from it,\n",
    "            run Viterbi on each sentence, generate the HMM predictions for each document,\n",
    "            compare this to the actual tags to calculate metrics\n",
    "    '''\n",
    "\n",
    "    tags_correct = 0.\n",
    "    given_tags = \"\"\n",
    "    generated_tags = \"\"\n",
    "\n",
    "    for doc in val_set: # for each file in the validation set\n",
    "        sentences = val_set[doc][0]\n",
    "        actual_tags = val_set[doc][1]\n",
    "\n",
    "        for i in range(len(sentences)): # for each sentence, run viterbi on sentence and compare output to actual tags\n",
    "            sent_words = sentences[i]\n",
    "            sent_tags = \"\".join(actual_tags[i])\n",
    "            [sequence, prob] = viterbi(T,E,initial,state_names,sent_words, trained_len)\n",
    "            given_tags += sent_tags\n",
    "            generated_tags += sequence\n",
    "\n",
    "    tp = 0.\n",
    "    fp = 0.\n",
    "    fn = 0.\n",
    "\n",
    "    for i in range(len(given_tags)):\n",
    "        if given_tags[i] == generated_tags[i]:\n",
    "            tags_correct += 1\n",
    "            if generated_tags[i] != 'O': # tp if tag != 'O'\n",
    "                tp += 1\n",
    "        else:\n",
    "            if generated_tags[i] != 'O': # fp if generated != O when correct does\n",
    "                fp += 1\n",
    "            else:     # fn if generated tag = 'O' when real tag does not\n",
    "                fn += 1\n",
    "\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "    \n",
    "    P = tp / (tp + fp)\n",
    "    R = tp / (tp + fn)\n",
    "    F = (2 * P * R) / (P + R)\n",
    "\n",
    "#     print \"True positives:\", tp\n",
    "#     print \"False positives:\", fp\n",
    "#     print \"False negatives:\", fn\n",
    "\n",
    "#     print \"Percentage of correct tags:\", tags_correct / len(given_tags)\n",
    "#     print \"Precision:\", P\n",
    "#     print \"Recall:\", R\n",
    "#     print \"F-score:\", F\n",
    "\n",
    "# average scores over all trials\n",
    "print \"\\nCalculated over %s trials ---------\" % trials\n",
    "\n",
    "print \"Average true positives:\", total_tp / trials\n",
    "print \"Average false positives:\", total_fp / trials\n",
    "print \"Average false negatives:\", total_fn / trials\n",
    "\n",
    "P = tp / (tp + fp)\n",
    "R = tp / (tp + fn)\n",
    "F = (2 * P * R) / (P + R)\n",
    "\n",
    "print \"Percentage of correct tags:\", tags_correct / len(given_tags)\n",
    "print \"Precision:\", P\n",
    "print \"Recall:\", R\n",
    "print \"F-score:\", F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "public_input = 'test-public/'\n",
    "public_output = 'test-public-edited/'\n",
    "private_input = 'test-private/'\n",
    "private_output = 'test-private-edited/'\n",
    "    \n",
    "directory = os.path.join(public_input)\n",
    "\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            public_sequences = get_sequences(public_input+file,T,E,initial,state_names,trained_len)\n",
    "            write_sequences(public_sequences, public_input+file, public_output+file)\n",
    "\n",
    "directory = os.path.join(private_input)            \n",
    "\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            private_sequences = get_sequences(private_input+file,T,E,initial,state_names,trained_len)\n",
    "            write_sequences(private_sequences, private_input+file, private_output+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "public_indices, public_sent_indices = return_indices(public_output)\n",
    "private_indices, private_sent_indices = return_indices(private_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "public_range_list = list(bl.create_ranges(public_indices))\n",
    "private_range_list = list(bl.create_ranges(private_indices))\n",
    "public_formatted = \" \".join(bl.format_ranges(public_range_list))\n",
    "private_formatted = \" \".join(bl.format_ranges(private_range_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sequence_predictions.csv', 'wb') as output:\n",
    "    output = csv.writer(output)\n",
    "    output.writerow(['Type', 'Spans'])\n",
    "    output.writerow(['CUE-public', public_formatted])\n",
    "    output.writerow(['CUE-private', private_formatted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentence_predictions.csv', 'wb') as output:\n",
    "    output = csv.writer(output)\n",
    "    output.writerow(['Type', 'Indices'])\n",
    "    output.writerow(['SENTENCE-public', \" \".join([str(i) for i in public_sent_indices])])\n",
    "    output.writerow(['SENTENCE-private', \" \".join([str(i) for i in private_sent_indices])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
